{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Named Entities: Prerequisites\n",
    "Author: Pierre Nugues\n",
    "\n",
    "__You must execute this notebook before your start the assignment.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the assignment is to create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2003 dataset. \n",
    "\n",
    "In this part, you will collect the datasets and the files you need to train your models. You will also collect the script you need to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a Training and a Test sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As annotated data and annotation scheme, you will use the data created for [CoNLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
    "1. Read the description of the CoNLL 2003 task\n",
    "2. Download both the training, validation, and test sets from https://data.deepai.org/conll2003.zip and decompress them. See the instructions below\n",
    "3. Note that the tagging scheme has been changed to IOB2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-27 11:14:26--  https://data.deepai.org/conll2003.zip\n",
      "Resolving data.deepai.org (data.deepai.org)... 138.199.37.229\n",
      "Connecting to data.deepai.org (data.deepai.org)|138.199.37.229|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 982975 (960K) [application/zip]\n",
      "Saving to: 'conll2003.zip'\n",
      "\n",
      "conll2003.zip       100%[===================>] 959.94K  3.49MB/s    in 0.3s    \n",
      "\n",
      "2024-04-27 11:14:26 (3.49 MB/s) - 'conll2003.zip' saved [982975/982975]\n",
      "\n",
      "Archive:  conll2003.zip\n",
      "  inflating: metadata                \n",
      "  inflating: test.txt                \n",
      "  inflating: train.txt               \n",
      "  inflating: valid.txt               \n",
      "mkdir: conll2003: File exists\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.deepai.org/conll2003.zip\n",
    "!unzip -u conll2003.zip\n",
    "!mkdir conll2003\n",
    "!mv train.txt valid.txt test.txt conll2003\n",
    "!rm conll2003.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation script\n",
    "\n",
    "You will train the models with the training set and the test set to evaluate them. For this, you will apply the `conlleval` script that will compute the harmonic mean of the precision and recall: F1. \n",
    "\n",
    "`conlleval` was written in Perl. Some people rewrote it in Python and you will use such such a translation in this lab. The line below installs it. The source code is available from this address: https://github.com/kaniblu/conlleval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conlleval\n",
      "  Downloading conlleval-0.2-py3-none-any.whl.metadata (171 bytes)\n",
      "Downloading conlleval-0.2-py3-none-any.whl (5.4 kB)\n",
      "Installing collected packages: conlleval\n",
      "Successfully installed conlleval-0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will represent the words with dense vectors, instead of a one-hot encoding. GloVe embeddings is one such representation. The Glove files contain a list of words, where each word is represented by a vector of a fixed dimension. In this notebook, we will use the file of 400,000 lowercase words with the 50 and 100-dimensional vectors.\n",
    "Download either:\n",
    "*  The GloVe embeddings 6B from <a href=\"https://nlp.stanford.edu/projects/glove/\">https://nlp.stanford.edu/projects/glove/</a> and keep the 50d and 100d vectors; or\n",
    "* A local copy of this dataset with the cell below (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-27 11:16:42--  https://fileadmin.cs.lth.se/nlp/nobackup/embeddings/nobackup/glove.6B.100d.txt.gz\n",
      "Resolving fileadmin.cs.lth.se (fileadmin.cs.lth.se)... 130.235.16.7\n",
      "Connecting to fileadmin.cs.lth.se (fileadmin.cs.lth.se)|130.235.16.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 134409071 (128M) [application/x-gzip]\n",
      "Saving to: 'glove.6B.100d.txt.gz'\n",
      "\n",
      "glove.6B.100d.txt.g 100%[===================>] 128.18M  43.1MB/s    in 3.0s    \n",
      "\n",
      "2024-04-27 11:16:45 (43.1 MB/s) - 'glove.6B.100d.txt.gz' saved [134409071/134409071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://fileadmin.cs.lth.se/nlp/nobackup/embeddings/nobackup/glove.6B.100d.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-27 11:17:24--  https://fileadmin.cs.lth.se/nlp/nobackup/embeddings/nobackup/glove.6B.50d.txt.zip\n",
      "Resolving fileadmin.cs.lth.se (fileadmin.cs.lth.se)... 130.235.16.7\n",
      "Connecting to fileadmin.cs.lth.se (fileadmin.cs.lth.se)|130.235.16.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 69240158 (66M) [application/zip]\n",
      "Saving to: 'glove.6B.50d.txt.zip'\n",
      "\n",
      "glove.6B.50d.txt.zi 100%[===================>]  66.03M  41.5MB/s    in 1.6s    \n",
      "\n",
      "2024-04-27 11:17:26 (41.5 MB/s) - 'glove.6B.50d.txt.zip' saved [69240158/69240158]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://fileadmin.cs.lth.se/nlp/nobackup/embeddings/nobackup/glove.6B.50d.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.50d.txt.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: __MACOSX/._glove.6B.50d.txt  \n"
     ]
    }
   ],
   "source": [
    "!gunzip -k glove.6B.100d.txt.gz\n",
    "!unzip -u glove.6B.50d.txt.zip\n",
    "!mkdir glove\n",
    "!mv glove.6B.100d.txt glove\n",
    "!mv glove.6B.50d.txt glove\n",
    "!rm glove.6B.100d.txt.gz glove.6B.50d.txt.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
